# Introduction

This project was to solve a task provided by a lesson named Database Programming Interfaces. The goal was to develop a simple search engine with a crawler which crawls title, url and most important words of a page (do stopword elimination).

# Run the stuff

As of now, the app consists of these components:

1. The Crawler, written in python
2. The Frontend/App, implemented as a HTML template which will be served via Python Framework Flask.
3. The DB which will be used to CRUD the results.

This project is being powered by docker - all images are public available at docker hub.

You can pull web and db via executing

```
docker pull 935610/webcrawler_web
docker pull 935610/webcrawler_crawler
```

And then expose them like shown in docker-compose.

Please make sure that they are exposed to ports which aren't already in use!

# Crawler

## Making things clear

When developing the crawler, I've been trying to develop two things in parallel for a long time.
The first approach was to build a crawler more from scratch, just using a combination of requests libs and BeautifulSoup. You can find the implementation in _crawler/crawler.py_.

## Where you can find the current implementation

The second and then focused approach was the more abstract, but way better solution via scrapy. The file structure is mostly generated by scrapy which is basically organized in projects. The project used here has been named _crawler-1_ (cool name, isn't it?). In a project there are some config files, you can store the requested html files there if you'd like to (I did it, dir named _saved-pages_).
The actual source code can be find in the subfolder which is named like the project, _crawler-1_. There you can find some more more advanced settings there in _settings.py_. As an example, you can define whether to obey the robots guidelines of the crawled page, or configure throttling so that you won't be blocked instantly on pages with good firewalls.

To understand the crawler, to us it's important to understand only these two things:

### Spider (the one and only)

Since one spider is fairly enough (remember, some people are afraid of them!) to crawl only a handful of pages, the spider simple_spider does the job. In there, you define the START_URLS, how deep the algo should go into the endless internet and what to do with the response - which parts of the page we're looking for. I kept it pretty simple, removed style, script and head and just took the content of all paragraphs. This is actually a bit too simple, but then you will get some data to work with if you're crawling some blogs, encyclopedias, that pages.

### Pipeline

Pipelines are there to process the data yielded by our spiders. Our pipeline does some stopword-elimination - stores url, title, original content of all paragraphs and the 'essence' of each text.

The reason do to a commit after each insertion is to make realtime-counting of indexed pages possible.

## Search engine

The search engine did took a bit more time. But let's explain the two pages:

### Home

On the home page, you just have a very clean page with an input field, a nice (freely available) background image and the (rough) number of indexed pages. On the one hand, this is for marketing, on the other hand for monitoring purposes to make sure the crawler is working in the background.

### Search results

If you have entered your word (and once again almost destroyed your return key on the keyboard), you get to the results page. First, you will see how long the evaluation took (sorry Google, got that idea from you) and you will of course see your results.

On top of each result you will see a small line \_\_, its opacity indicates the score of that result. The darker the line is, the better the result.

If you scroll down the list, an input field will remain at the top of the page (again sorry @Google).
If there aren't any results, you will be asked to input other keywords.

If the keywords are in the text of the page, an excerpt will be shown with highlighted keyword.

One sentence to performance: currently the application doesn't scale properly. Up to 5000 pages there shouldn't be any problems (about 0-2s), but with tens of thousands of rows it could take some seconds.

Some improvements, which I was actually up to implement, but sadly I had the situation that two friends of mine recently died in two car accidents, so I was not as productive as I actually wanted to be.
The improvements would have been:

1. Better scoring for the pages (maybe using neural network for it?)
2. Pagination in results, set threshold for score to reduce runtime
3. Fancy result animation with help of CSS and AJAX
4. Live crawler monitoring with stats

But as of now, I'm also happy with the result.

Kind regards

Bernd
